{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a6ead1",
   "metadata": {},
   "source": [
    "# LBB: Building LLM Applications for Structured Data Insights Use RAG\n",
    "\n",
    "Muh Amri Sidiq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fa721",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "üîç What is an LLM (Large Language Model)?\n",
    "A Large Language Model is an advanced AI system trained on massive amounts of text data. It understands and generates human-like language, enabling it to:\n",
    " - Answer questions\n",
    " - Write and summarize content\n",
    " - Support dialogue and reasoning\n",
    " - Assist in data analysis, and more.\n",
    "\n",
    "LLMs include models like OpenAI‚Äôs GPT, Meta‚Äôs LLaMA, or Google‚Äôs Gemini.\n",
    "\n",
    "üîé What is RAG (Retrieval-Augmented Generation)?\n",
    "Retrieval-Augmented Generation (RAG) is a framework that combines:\n",
    " - Retrieval: Searching for relevant information from external data (e.g., documents, databases, company reports)\n",
    " - Generation: Using an LLM to generate answers based on that retrieved data.\n",
    "\n",
    "üìå When Do You Need RAG?\n",
    "You should consider using RAG when:\n",
    " - The LLM doesn't know your private/internal data (like sales reports, product records, etc.)\n",
    " - You need fact-based or evidence-grounded responses, not just general knowledge\n",
    "\n",
    "üíº Example: RAG with the US E-commerce Record 2020 Dataset\n",
    "Using the dataset from Kaggle: US E-commerce Record 2020, you can:\n",
    " - Let the LLM answer questions about customer trends, sales performance, and product behavior\n",
    " - Ensure that responses are based on actual transaction data instead of generic e-commerce knowledge\n",
    " - Build a chatbot or dashboard that answers queries like:\n",
    "\n",
    "   - ‚ÄúWhat was the average order value for returning customers in Texas?‚Äù\n",
    "   - ‚ÄúList the top 5 most sold product categories in December 2020.‚Äù\n",
    "   - ‚ÄúHow does sales performance differ between payment types?‚Äù\n",
    "\n",
    "This allows business users to interact with internal data conversationally, while ensuring accuracy and contextual relevance through retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4f27a",
   "metadata": {},
   "source": [
    "## Workflow RAG\n",
    "\n",
    "there are several steps to determine the RAG flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf1702",
   "metadata": {},
   "source": [
    "### 1. Read Data\n",
    "\n",
    "the data we use is tabular data that contains ecomerce transaction columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc88f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>849</td>\n",
       "      <td>CA-2017-107503</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>GA-14725</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Lorain</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>44052</td>\n",
       "      <td>East</td>\n",
       "      <td>FUR-FU-10003878</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Linden 10\" Round Wall Clock, Black</td>\n",
       "      <td>48.896</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8.5568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>4010</td>\n",
       "      <td>CA-2017-144463</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SC-20725</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036</td>\n",
       "      <td>West</td>\n",
       "      <td>FUR-FU-10001215</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Howard Miller 11-1/2\" Diameter Brentwood Wall ...</td>\n",
       "      <td>474.430</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.2606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>6683</td>\n",
       "      <td>CA-2017-154466</td>\n",
       "      <td>First Class</td>\n",
       "      <td>DP-13390</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>53132</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10002012</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Wilson Jones Easy Flow II Sheet Lifters</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>8070</td>\n",
       "      <td>CA-2017-151750</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>JM-15250</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77340</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-ST-10002743</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>SAFCO Boltless Steel Shelving</td>\n",
       "      <td>454.560</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-107.9580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>8071</td>\n",
       "      <td>CA-2017-151750</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>JM-15250</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77340</td>\n",
       "      <td>Central</td>\n",
       "      <td>FUR-FU-10002116</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Tenex Carpeted, Granite-Look or Clear Contempo...</td>\n",
       "      <td>141.420</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-187.3815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order Date  Row ID        Order ID       Ship Mode Customer ID      Segment  \\\n",
       "0   01-01-20     849  CA-2017-107503  Standard Class    GA-14725     Consumer   \n",
       "1   01-01-20    4010  CA-2017-144463  Standard Class    SC-20725     Consumer   \n",
       "2   01-01-20    6683  CA-2017-154466     First Class    DP-13390  Home Office   \n",
       "3   01-01-20    8070  CA-2017-151750  Standard Class    JM-15250     Consumer   \n",
       "4   01-01-20    8071  CA-2017-151750  Standard Class    JM-15250     Consumer   \n",
       "\n",
       "         Country         City       State  Postal Code   Region  \\\n",
       "0  United States       Lorain        Ohio        44052     East   \n",
       "1  United States  Los Angeles  California        90036     West   \n",
       "2  United States     Franklin   Wisconsin        53132  Central   \n",
       "3  United States   Huntsville       Texas        77340  Central   \n",
       "4  United States   Huntsville       Texas        77340  Central   \n",
       "\n",
       "        Product ID         Category Sub-Category  \\\n",
       "0  FUR-FU-10003878        Furniture  Furnishings   \n",
       "1  FUR-FU-10001215        Furniture  Furnishings   \n",
       "2  OFF-BI-10002012  Office Supplies      Binders   \n",
       "3  OFF-ST-10002743  Office Supplies      Storage   \n",
       "4  FUR-FU-10002116        Furniture  Furnishings   \n",
       "\n",
       "                                        Product Name    Sales  Quantity  \\\n",
       "0                 Linden 10\" Round Wall Clock, Black   48.896         4   \n",
       "1  Howard Miller 11-1/2\" Diameter Brentwood Wall ...  474.430        11   \n",
       "2            Wilson Jones Easy Flow II Sheet Lifters    3.600         2   \n",
       "3                      SAFCO Boltless Steel Shelving  454.560         5   \n",
       "4  Tenex Carpeted, Granite-Look or Clear Contempo...  141.420         5   \n",
       "\n",
       "   Discount    Profit  \n",
       "0       0.2    8.5568  \n",
       "1       0.0  199.2606  \n",
       "2       0.0    1.7280  \n",
       "3       0.2 -107.9580  \n",
       "4       0.6 -187.3815  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the e-commerce dataset\n",
    "# The dataset is assumed to be in the same directory as this script.\n",
    "ecommerce = pd.read_csv(\"data/US  E-commerce records 2020.csv\", encoding='cp1252')\n",
    "ecommerce.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2629f70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3312 entries, 0 to 3311\n",
      "Data columns (total 19 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Order Date    3312 non-null   object \n",
      " 1   Row ID        3312 non-null   int64  \n",
      " 2   Order ID      3312 non-null   object \n",
      " 3   Ship Mode     3312 non-null   object \n",
      " 4   Customer ID   3312 non-null   object \n",
      " 5   Segment       3312 non-null   object \n",
      " 6   Country       3312 non-null   object \n",
      " 7   City          3312 non-null   object \n",
      " 8   State         3312 non-null   object \n",
      " 9   Postal Code   3312 non-null   int64  \n",
      " 10  Region        3312 non-null   object \n",
      " 11  Product ID    3312 non-null   object \n",
      " 12  Category      3312 non-null   object \n",
      " 13  Sub-Category  3312 non-null   object \n",
      " 14  Product Name  3312 non-null   object \n",
      " 15  Sales         3312 non-null   float64\n",
      " 16  Quantity      3312 non-null   int64  \n",
      " 17  Discount      3312 non-null   float64\n",
      " 18  Profit        3312 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(13)\n",
      "memory usage: 491.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ecommerce.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c5351",
   "metadata": {},
   "source": [
    "### 2. Data to Text Transformation\n",
    "\n",
    "So that the data can be used, which was originally tabular, it must first be converted into text form, so that LLM can read it. By combining all the columns of each row, the first step is to make the data of each row into 1 full sentence as a condition for LLM to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac8e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat sebuah fungsi\n",
    "def merge_column(df, column_data):\n",
    "    df['teks'] = df[column_data].astype('str').agg(' | '.join, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1c9cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>teks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>849</td>\n",
       "      <td>CA-2017-107503</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>GA-14725</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Lorain</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>44052</td>\n",
       "      <td>East</td>\n",
       "      <td>FUR-FU-10003878</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Linden 10\" Round Wall Clock, Black</td>\n",
       "      <td>48.896</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8.5568</td>\n",
       "      <td>01-01-20 | 849 | CA-2017-107503 | Standard Cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>4010</td>\n",
       "      <td>CA-2017-144463</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SC-20725</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036</td>\n",
       "      <td>West</td>\n",
       "      <td>FUR-FU-10001215</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Howard Miller 11-1/2\" Diameter Brentwood Wall ...</td>\n",
       "      <td>474.430</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.2606</td>\n",
       "      <td>01-01-20 | 4010 | CA-2017-144463 | Standard Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>6683</td>\n",
       "      <td>CA-2017-154466</td>\n",
       "      <td>First Class</td>\n",
       "      <td>DP-13390</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>53132</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10002012</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Wilson Jones Easy Flow II Sheet Lifters</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7280</td>\n",
       "      <td>01-01-20 | 6683 | CA-2017-154466 | First Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>8070</td>\n",
       "      <td>CA-2017-151750</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>JM-15250</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77340</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-ST-10002743</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>SAFCO Boltless Steel Shelving</td>\n",
       "      <td>454.560</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-107.9580</td>\n",
       "      <td>01-01-20 | 8070 | CA-2017-151750 | Standard Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-01-20</td>\n",
       "      <td>8071</td>\n",
       "      <td>CA-2017-151750</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>JM-15250</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77340</td>\n",
       "      <td>Central</td>\n",
       "      <td>FUR-FU-10002116</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Tenex Carpeted, Granite-Look or Clear Contempo...</td>\n",
       "      <td>141.420</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-187.3815</td>\n",
       "      <td>01-01-20 | 8071 | CA-2017-151750 | Standard Cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order Date  Row ID        Order ID       Ship Mode Customer ID      Segment  \\\n",
       "0   01-01-20     849  CA-2017-107503  Standard Class    GA-14725     Consumer   \n",
       "1   01-01-20    4010  CA-2017-144463  Standard Class    SC-20725     Consumer   \n",
       "2   01-01-20    6683  CA-2017-154466     First Class    DP-13390  Home Office   \n",
       "3   01-01-20    8070  CA-2017-151750  Standard Class    JM-15250     Consumer   \n",
       "4   01-01-20    8071  CA-2017-151750  Standard Class    JM-15250     Consumer   \n",
       "\n",
       "         Country         City       State  Postal Code   Region  \\\n",
       "0  United States       Lorain        Ohio        44052     East   \n",
       "1  United States  Los Angeles  California        90036     West   \n",
       "2  United States     Franklin   Wisconsin        53132  Central   \n",
       "3  United States   Huntsville       Texas        77340  Central   \n",
       "4  United States   Huntsville       Texas        77340  Central   \n",
       "\n",
       "        Product ID         Category Sub-Category  \\\n",
       "0  FUR-FU-10003878        Furniture  Furnishings   \n",
       "1  FUR-FU-10001215        Furniture  Furnishings   \n",
       "2  OFF-BI-10002012  Office Supplies      Binders   \n",
       "3  OFF-ST-10002743  Office Supplies      Storage   \n",
       "4  FUR-FU-10002116        Furniture  Furnishings   \n",
       "\n",
       "                                        Product Name    Sales  Quantity  \\\n",
       "0                 Linden 10\" Round Wall Clock, Black   48.896         4   \n",
       "1  Howard Miller 11-1/2\" Diameter Brentwood Wall ...  474.430        11   \n",
       "2            Wilson Jones Easy Flow II Sheet Lifters    3.600         2   \n",
       "3                      SAFCO Boltless Steel Shelving  454.560         5   \n",
       "4  Tenex Carpeted, Granite-Look or Clear Contempo...  141.420         5   \n",
       "\n",
       "   Discount    Profit                                               teks  \n",
       "0       0.2    8.5568  01-01-20 | 849 | CA-2017-107503 | Standard Cla...  \n",
       "1       0.0  199.2606  01-01-20 | 4010 | CA-2017-144463 | Standard Cl...  \n",
       "2       0.0    1.7280  01-01-20 | 6683 | CA-2017-154466 | First Class...  \n",
       "3       0.2 -107.9580  01-01-20 | 8070 | CA-2017-151750 | Standard Cl...  \n",
       "4       0.6 -187.3815  01-01-20 | 8071 | CA-2017-151750 | Standard Cl...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Order Date', 'Row ID', 'Order ID', 'Ship Mode', 'Customer ID',\n",
    "        'Segment', 'Country', 'City', 'State', 'Postal Code',\n",
    "        'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name',\n",
    "        'Sales', 'Quantity', 'Discount', 'Profit']\n",
    "\n",
    "merge_column(df=ecommerce, column_data=cols)\n",
    "ecommerce.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34ed2f",
   "metadata": {},
   "source": [
    "### 3. Search for Text Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a20ab0",
   "metadata": {},
   "source": [
    "#### 3.1 Embedding Teks\n",
    "\n",
    "At this stage we convert the text into a vector representation capturing the meaning of the text. In this case looking for similarities between one data and another.\n",
    "\n",
    "Information about the similarity between one data and another is important because the initial goal of RAG is to retrieve data information that is relevant to the given question, in order to generate relevant LLM answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619997ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the embedding model to be used\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d2c0b",
   "metadata": {},
   "source": [
    "`query =  \"what segment makes the most purchases in the state of California?\"`\n",
    "\n",
    "`embedding_query_new = model.encode(query, convert_to_numpy=True)`\n",
    "\n",
    "Setting up a query and embedding it for the information retrieval process.\n",
    "The purpose of this code is to convert the text question (\"query\"). into a vector representation (embedding) so that it can be compared with data already indexed by FAISS. With this representation, the system can search for the most relevant rows of data based on similarity of meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8c06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare query & embedding new\n",
    "query =  \"what segment makes the most purchases in the state of California?\"\n",
    "embedding_query_new = model.encode(query, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049f29a",
   "metadata": {},
   "source": [
    "`embedding_dataframe = model.encode(ecommerce['teks'], convert_to_numpy=True)`\n",
    "\n",
    "This code serves to convert the text in the text column of the ecommerce DataFrame into a vector representation (embedding) using the SentenceTransformer model. This embedding is important for performing semantic search with FAISS\n",
    " - model.encode(...): Calls the model to convert the text to embedding.\n",
    " - ecommerce[‚Äòtext‚Äô]: A column containing combined information from multiple\n",
    " - columns, prepared beforehand with a function like transform_data.\n",
    "\n",
    "convert_to_numpy=True: Sets the output to be in numpy array format, which is required for use in FAISS index or other mathematical operations. The main purpose of this step is to prepare the data so that it can be searched based on similarity of meaning, not just keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ffb08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding for our data\n",
    "embedding_dataframe = model.encode(ecommerce['teks'], convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677f6de",
   "metadata": {},
   "source": [
    "`cosine_scores = util.cos_sim(embedding_query_new, embedding_dataframe)`\n",
    "\n",
    "This code is used to calculate the semantic similarity between the query and all data in the embedding form.\n",
    "\n",
    "Details:\n",
    " - embedding_query_new: The representation vector of the query that has been converted to embedding.\n",
    " - embedding_dataframe: Set of embeddings of all text data in the text column of the DataFrame.\n",
    " - util.cos_sim(...): A function from sentence_transformers.util that calculates cosine similarity, a measure of how similar two direction vectors are in multidimensional space.\n",
    "\n",
    "Main objective: Find out how relevant each line of data is to the user's question based on closeness of meaning, not just word similarity.\n",
    "\n",
    "Output:\n",
    " - cosine_scores produces a matrix with values between -1 and 1.\n",
    " - A value close to 1 means very similar, close to 0 means not similar, and a negative value means very different in semantic direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbb1e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1494, 0.2476, 0.0978,  ..., 0.3054, 0.3561, 0.2677]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate cosine similarity\n",
    "cosine_scores = util.cos_sim(embedding_query_new, embedding_dataframe)\n",
    "cosine_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea173106",
   "metadata": {},
   "source": [
    "#### 3.2: FAISS Indexing From Question\n",
    "\n",
    "`import faiss, import numpy as np` This line imports two important libraries:\n",
    " - `faiss`: Used for indexing and similarity-based vector search.\n",
    " - `numpy`: Used for numeric array manipulation, such as vector normalization.\n",
    "\n",
    "`embedding_dataframe = embedding_dataframe / np.linalg.norm(embedding_dataframe, axis=1, keepdims=True)`\n",
    "`embedding_dataframe = embedding_dataframe.astype('float32')`\n",
    "Manual normalization of embedding\n",
    " - Normalization is done so that each embedding vector has a length (norm) = 1.\n",
    " - This is important because FAISS uses inner product (dot product) to calculate similarity, and when vectors are normalized, dot product = cosine similarity\n",
    " - astype(‚Äòfloat32‚Äô) is required because FAISS only accepts float32 data type.\n",
    "   \n",
    "   üîç This normalization ensures the similarity search in FAISS is equivalent to cosine similarity.\n",
    "\n",
    "`dimension = embedding_dataframe.shape[1]` Specifying Embedding Dimensions\n",
    " - Retrieves the number of dimensions of the embedding (for example, 384 if using the 'paraphrase-MiniLM-L6-v2' model).\n",
    " - This value is needed when creating the FAISS index:\n",
    "    This dimension is the size of the vector space where all embeddings are placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b3dbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the cosine similarity scores with manually\n",
    "embedding_dataframe = embedding_dataframe / np.linalg.norm(embedding_dataframe, axis=1, keepdims=True)\n",
    "embedding_dataframe = embedding_dataframe.astype('float32')\n",
    "\n",
    "# take value embedding\n",
    "dimension = embedding_dataframe.shape[1]\n",
    "dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19fb9ab",
   "metadata": {},
   "source": [
    "`index = faiss.IndexFlatL2(dimension)` Creating and Populating the FAISS Index\n",
    "\n",
    "This line makes the FAISS index use L2 (Euclidean) distance as the similarity metric.\n",
    "Explanation:\n",
    " - IndexFlatL2 is a FAISS index type that calculates the Euclidean distance (L2 distance) between vectors.\n",
    " - dimension is the number of dimensions of each embedding vector (e.g. 384).\n",
    " - This index does not use fast search structures (such as IVF or HNSW) - suitable for small to medium datasets.\n",
    "\n",
    "   üìå Use IndexFlatIP if you want to calculate cosine similarity with normalized vectors.\n",
    "\n",
    "`index.add(embedding_dataframe)` This line adds all embedding vectors to the FAISS index.\n",
    "Explanation:\n",
    " - embedding_dataframe contains all the encoded vectors of the text in the dataset.\n",
    " - Once added, the index is ready to be used for similarity-based queries.\n",
    "\n",
    "   FAISS can now be used to find the most similar data to the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011136e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb544d",
   "metadata": {},
   "source": [
    "The dimension results if compared back to the example query question above, will be as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd2c2a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what segment makes the most purchases in the state of California?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9deb26",
   "metadata": {},
   "source": [
    "`embedding_index_query = model.encode([query])`\n",
    "\n",
    "Explanation:\n",
    " - This code is used to convert a user's query into an embedding vector, so that it can be compared with vectors of previously indexed data.\n",
    " - `model.encode(...)`: A function of the SentenceTransformer that converts text into a numerical representation (embedding).\n",
    " - `[query]`: Given in list form because the model expects the input to be a list of strings, even if it is only one query.\n",
    " - The result is a 2-dimensional array (shape: [1, dimension]), which is the format FAISS needs for the search process.\n",
    " \n",
    " Main objective: Convert the user's question into a vector format, so that it can be compared against the entire data to find the most relevant answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_query = model.encode([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d91af",
   "metadata": {},
   "source": [
    "`D, I = index.search(embedding_index_query, k = 2)`\n",
    "\n",
    "Explanation:\n",
    " - This code is used to find the k closest (most similar) vectors in the FAISS index to the query that has been converted to embedding.\n",
    " - `embedding_index_query`: The encoding result vector of the user query.\n",
    " - `k = 2`: Retrieve the 2 closest results (top-2 most similar).\n",
    " - `index.search(...)`: FAISS function to perform a distance-based search (e.g. Euclidean if using IndexFlatL2, or cosine if using IndexFlatIP + normalization).\n",
    "\n",
    "Output:\n",
    " - `D`: The distance matrix or similarity score between the query and the k closest results.\n",
    " - `I`: The row index matrix of the original data that is most similar to the query.\n",
    "\n",
    "Main objective: Find the rows of data in the dataset that are most relevant to the user's query based on embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2323e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(embedding_index_query, k = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf46a4d",
   "metadata": {},
   "source": [
    "`D:` the distance between the query_vector and the nearest vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "148f592e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.061226, 44.409435]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f143b0dd",
   "metadata": {},
   "source": [
    "`I:` index of most similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4415f26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3178,  411]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95673be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1494, 0.2476, 0.0978,  ..., 0.3054, 0.3561, 0.2677]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef329530",
   "metadata": {},
   "source": [
    "Function: Build FAISS Index with Cosine Similarity\n",
    "\n",
    "Function Explanation:\n",
    "The build_faiss_index_cosine(text) function is used to create a FAISS index based on cosine similarity, which is useful in the process of searching relevant data based on the similarity of text meaning.\n",
    "\n",
    "Steps:\n",
    "1. Embedding Text: `embedding = model.encode(teks , convert_to_numpy=True)`\n",
    "   Convert a text list (text) into a numeric vector (embedding) using SentenceTransformer.\n",
    "2. Normalization for Cosine Similarity: `embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)`,\n",
    "`embedding = embedding.astype('float32')`. \n",
    "   - Normalize each vector to unit-norm so that dot product = cosine similarity.  - Convert to float32 as FAISS only supports this data type.\n",
    "3. Create and Fill Index:\n",
    "   `index = faiss.IndexFlatL2(dimension)`\n",
    "    `index.add(embedding)`\n",
    "   - Creating FAISS index is based on L2 distance, but since the vectors are already normalized, the L2 search is equivalent to cosine similarity.\n",
    "   - dimension is the number of dimensions of the embedding vector (e.g. 384 or 768 depending on the model).\n",
    "4. Return:\n",
    "   - Returns the FAISS index and its embedding for use in the search process.\n",
    "\n",
    "Notes:\n",
    "- FAISS does not provide IndexFlatCosine function, so to do cosine similarity, we need to do manual normalization and still use IndexFlatL2.\n",
    "- Make sure the input text is in the form of a list of strings so that the encode runs smoothly.\n",
    "\n",
    "The main purpose of this function: Build a meaning-based fast search structure for use in RAG (Retrieval-Augmented Generation) or data-driven question and answer systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43bc186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to build FAISS index with cosine similarity\n",
    "def build_faiss_index_cosine(teks):\n",
    "    # Section for performing embeddings\n",
    "    embedding = model.encode(teks , convert_to_numpy=True)\n",
    "\n",
    "    # Perform cosine calculation\n",
    "    embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)\n",
    "    embedding = embedding.astype('float32')\n",
    "\n",
    "    # Indexing\n",
    "    dimension = embedding.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embedding)\n",
    "\n",
    "    return index, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b188934",
   "metadata": {},
   "source": [
    "Function: Retrieve Relevant Data from FAISS Index\n",
    "\n",
    "Function Explanation:\n",
    "The retrieve() function is used to retrieve the most relevant rows of data against a query based on meaning similarity using the FAISS index.\n",
    "\n",
    "Detailed Steps:\n",
    "1. Encode and Normalize Query:\n",
    "\n",
    "   `query_embedding = model.encode([query], convert_to_numpy=True)`\n",
    "   `query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)`\n",
    "   `query_embedding = query_embedding.astype(\"float32\")`\n",
    "\n",
    "   - The query is converted into a vector (embedding).\n",
    "   - The query vector is normalized so that it can be compared with the embedding data based on cosine similarity.\n",
    "   - Conversion to float32 as FAISS only supports this format.\n",
    "2. Search the FAISS Index:\n",
    "   \n",
    "   `scores, indices = index.search(query_embedding, top_k)`\n",
    "\n",
    "   - Searches the index to find the top_k most similar results.\n",
    "   - scores: Similarity or distance values (higher if cosine similarity).\n",
    "   - indices: Index of rows from the original data that are most relevant.\n",
    "3. Fetch Data from DataFrame:\n",
    "   \n",
    "   `result_df = df.iloc[indices[0]].copy()`\n",
    "   `result_df['similarity_score'] = scores[0]`\n",
    "\n",
    "   - Retrieve rows from the original DataFrame (df) based on the FAISS result.\n",
    "   - index.- Adds a new column similarity_score to display the similarity score.\n",
    "4. Return Results:\n",
    "   \n",
    "   `return result_df`\n",
    "\n",
    "   - Returns a DataFrame containing relevant data rows and similarity scores.\n",
    "\n",
    "Final Destination:\n",
    "This function is the core part of the RAG (Retrieval-Augmented Generation) system, which enables the model to answer the most relevant data-driven questions quickly and accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83599207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to retrieve data from FAISS index\n",
    "def retrieve(query, index, df, top_k=3):\n",
    "    # 1. Encode dan normalization query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "    query_embedding = query_embedding.astype(\"float32\")\n",
    "\n",
    "    # 2. Search to FAISS\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # 3. Retrieve the data row according to the indexing result\n",
    "    result_df = df.iloc[indices[0]].copy()\n",
    "    result_df['similarity_score'] = scores[0]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951593c",
   "metadata": {},
   "source": [
    "Function: Generate Answers Using OpenAI GPT\n",
    "\n",
    "Function Explanation:\n",
    "The generate_answer() function is used to generate text-based answers using the OpenAI ChatCompletion API (GPT-4.1-mini model). It combines the user's question and relevant context data to generate an informative and specific answer.\n",
    "\n",
    " Line-by-Line Explanation:\n",
    "1. Set the OpenAI API Key:\n",
    "   `openai.api_key = api_key`\n",
    "\n",
    "   The API key is set so that the request can be authenticated and the OpenAI service can be used.\n",
    "2. Set up a Prompt System:\n",
    "\n",
    "   `system_message = \"Kamu adalah asisten cerdas yang menjawab pertanyaan berdasarkan data yang diberikan.\"`\n",
    "   \n",
    "   System messages to direct the model to act as a data-driven intelligent assistant.\n",
    "3. Compose Prompts from Users:\n",
    "   \n",
    "   `user_message = f\"\"\"`\n",
    "   `Pertanyaan: {query}`\n",
    "\n",
    "   `Data yang relevan:`\n",
    "\n",
    "   `{context}`\n",
    "   \n",
    "   `\"\"\"`\n",
    "\n",
    "   Combining questions and relevant data (results from FAISS retrieval) into a prompt format for the model.\n",
    "\n",
    "4. Send Request to OpenAI:\n",
    "   \n",
    "   `response = openai.ChatCompletion.create(`\n",
    "\n",
    "    `model=\"gpt-4.1-mini\",`\n",
    "    \n",
    "    `messages=[...],`\n",
    "    \n",
    "    `temperature=0.3,`\n",
    "    \n",
    "    `max_tokens=1000`\n",
    "    \n",
    "    `)`\n",
    "\n",
    "    - Model: The model used (in this case GPT-4.1-mini).\n",
    "    - messages: A list of messages to set the context of the conversation. \n",
    "    - temperature=0.3: Creativity control. The lower it is, the more deterministic the output.\n",
    "    - max_tokens=1000: Limit on the length of the generated answer.\n",
    "\n",
    "5. Returning Answers:\n",
    "   \n",
    "   `return response.choices[0].message[\"content\"]`\n",
    "\n",
    "   Retrieve the answer from the result provided by OpenAI and return it as text.\n",
    "\n",
    "Function Objective:\n",
    "Connecting the data retrieval process with GPT's reasoning capabilities, so that the system can answer questions based on semantically discovered information from the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e0b2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_answer(query, context, api_key):\n",
    "    # to enter the api key of the generative model being used\n",
    "    openai.api_key = api_key\n",
    "    # to tell you specifically what the generative model needs to do.\n",
    "    system_message = \"Kamu adalah asisten cerdas yang menjawab pertanyaan berdasarkan data yang diberikan.\"\n",
    "    # for users to input questions or data that they want to learn.\n",
    "    user_message = f\"\"\"\n",
    "    Pertanyaan: {query}\n",
    "\n",
    "    Data yang relevan:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4.1-mini\", # the model being used\n",
    "        # system messages or to process the input data or user\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        # to adjust the level of randomness in the next word prediction\n",
    "        temperature=0.3,\n",
    "        # to set the maximum number of tokens that can be processed\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f28d9e",
   "metadata": {},
   "source": [
    "Generate Answers Using GPT Based on Relevant Data\n",
    "\n",
    "Explanation:\n",
    "The generate_answer() function is called to answer a user-specific question based on the data set provided in the text fields of the ecommerce DataFrame.\n",
    "Parameters:\n",
    " - `query`:\n",
    "   The user question that the model wants to answer.\n",
    "   In this example:\n",
    "   ‚Äúwhat segment makes the most purchases in the state of California?‚Äù\n",
    "\n",
    "- `context`:\n",
    "   A set of relevant data in text form (usually the result of multiple columns combined), used as context for the GPT model to answer based on real information.\n",
    "   Here: ecommerce[‚Äòtext‚Äô].\n",
    "\n",
    "- `api_key`:\n",
    "  The API key used to access the OpenAI ChatCompletion API. Must be provided for the request to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d80add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data yang diberikan hanya berupa sebagian kecil dari dataset dan tidak mencakup informasi lengkap mengenai kolom-kolom seperti Ship Mode, Segment, dan nilai Sales secara eksplisit. Oleh karena itu, saya tidak dapat menghitung total penjualan (total sales) untuk kondisi State = CA, Ship Mode = First Class, dan Segment = Home Office hanya berdasarkan data yang tersedia.\\n\\nJika Anda dapat menyediakan data lengkap yang mencakup kolom State, Ship Mode, Segment, dan nilai Sales, saya dapat membantu menghitung total penjualan sesuai kriteria tersebut.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(query= \"What is the total sales for State = CA, Ship Mode = First Class, and Segment = Home Office?\",\n",
    "                context=  ecommerce['teks'],  \n",
    "                api_key= \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484feeb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through a series of code builds, we successfully implemented an intelligent and interactive Retrieval-Augmented Generation (RAG) system, capable of answering user queries based on 2020 e-commerce data. The system combines the strengths of:\n",
    "\n",
    "Key Technical Components:\n",
    "1. Data Preprocessing & Transformation\n",
    "   - E-commerce data is read from CSV files with special encoding (cp1252) and combined into a single text column (text) for semantic analysis.\n",
    "\n",
    "2. Embedding & Indexing\n",
    "   - Using SentenceTransformer model (paraphrase-MiniLM-L6-v2) to transform text into embedding vectors.\n",
    "   - Embedding is normalized to simulate cosine similarity using FAISS (IndexFlatL2).\n",
    "   - FAISS is used to build the vector index and perform a quick search for meaning similarity.\n",
    "\n",
    "2. Semantic Retrieval\n",
    "   - User queries are converted into embedding vectors and matched against the index to find the most relevant (top-k) data.\n",
    "   - The most similar data rows are returned with a similarity score.\n",
    "\n",
    "3. Natural Language Answers (LLM)\n",
    "   - Using OpenAI GPT-4.1-mini, the system forms natural answers based on the context of the found data.\n",
    "   - Prompts consist of system messages and user messages that include the question and context of the retrieval results.\n",
    "\n",
    "4. Interactive Interface (UI)\n",
    "   - With Streamlit, users can upload CSV files, select fields, type questions, and view answers directly in one application page.\n",
    "\n",
    "Benefits and Objectives\n",
    "This system enables:\n",
    " - Data-driven question answering without having to perform SQL queries or manual exploration.\n",
    " - Easy integration with new datasets by simply uploading CSV files.\n",
    " - Combining the power of search engine (FAISS) with generative reasoning (GPT), ideal for business insight, customer analytics, and large text analysis.\n",
    "\n",
    "Next Steps (Optional)\n",
    " - Add answer highlights or traceback.\n",
    " - Implement data filtering before indexing (e.g. only rows from California). \n",
    " - Implement query result storage or cache with Streamlit session.\n",
    "\n",
    "With this system, you have built a solid foundation for an AI assistant based on internal company data - extendable to HR data, finance, logistics, or even other free text reports."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
